{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13de9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import weaviate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Configure\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f37e91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_embedding(text):\n",
    "    \"\"\"Obtener embedding de Ollama directamente\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/embeddings\",\n",
    "            json={\n",
    "                \"model\": \"qwen3:4b\",  # Usar el modelo que tienes disponible\n",
    "                \"prompt\": text\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"embedding\"]\n",
    "        else:\n",
    "            print(f\"Error en Ollama: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error conectando a Ollama: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_new_file(file_path, collection_name):\n",
    "    \"\"\"\n",
    "    Carga un nuevo documento PDF a una colecci√≥n existente de Weaviate\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo PDF\n",
    "        collection_name (str): Nombre de la colecci√≥n existente\n",
    "    \"\"\"\n",
    "    # Conectar a Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "    print(\"‚úÖ Conectado a Weaviate local\")\n",
    "    \n",
    "    # Verificar que el archivo existe\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {file_path}\")\n",
    "    \n",
    "    # Verificar que la colecci√≥n existe\n",
    "    if not client.collections.exists(collection_name):\n",
    "        raise ValueError(f\"La colecci√≥n '{collection_name}' no existe. Cr√©ala primero.\")\n",
    "    \n",
    "    # Obtener la colecci√≥n existente\n",
    "    collection = client.collections.get(collection_name)\n",
    "    print(f\"üì¶ Conectado a la colecci√≥n '{collection_name}'\")\n",
    "    \n",
    "    # Cargar el PDF\n",
    "    print(f\"üìÑ Cargando PDF: {file_path}\")\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Dividir el texto en chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=350,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "    print(f\"üìù Documento dividido en {len(chunked_documents)} chunks\")\n",
    "    \n",
    "    # Obtener el nombre del archivo para el source\n",
    "    file_name = Path(file_path).name\n",
    "    \n",
    "    # Obtener el siguiente chunk_id disponible (para no duplicar IDs)\n",
    "    # Consultar todos los documentos para obtener el chunk_id m√°s alto\n",
    "    existing_docs = collection.query.fetch_objects(\n",
    "        limit=10000  # Ajustar seg√∫n tus necesidades\n",
    "    )\n",
    "    max_chunk_id = -1\n",
    "    for obj in existing_docs.objects:\n",
    "        if 'chunk_id' in obj.properties:\n",
    "            max_chunk_id = max(max_chunk_id, obj.properties['chunk_id'])\n",
    "    \n",
    "    starting_chunk_id = max_chunk_id + 1\n",
    "    print(f\"üî¢ Comenzando desde chunk_id: {starting_chunk_id}\")\n",
    "    \n",
    "    # Insertar documentos\n",
    "    print(\"üíæ Insertando documentos...\")\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, doc in enumerate(chunked_documents):\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"text\": doc.page_content,\n",
    "                    \"chunk_id\": starting_chunk_id + i,\n",
    "                    \"source\": file_name,\n",
    "                    \"length\": len(doc.page_content)\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    print(f\"‚úÖ {len(chunked_documents)} documentos insertados exitosamente\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas de la colecci√≥n\n",
    "    total_objects = collection.aggregate.over_all(total_count=True)\n",
    "    print(f\"üìä Total de documentos en la colecci√≥n: {total_objects.total_count}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0588943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jose\\Documents\\Programacion\\Chatbot_Langchain\\.venv\\Lib\\site-packages\\weaviate\\warnings.py:302: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_local()\n",
    "collection_name = \"DocumentosPDFOllama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DocumentosPDF': _CollectionConfigSimple(name='DocumentosPDF', description=None, generative_config=None, properties=[_Property(name='text', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='chunk_id', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='source', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='length', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer=None, vectorizer_configs={})], references=[], reranker_config=None, vectorizer_config=None, vectorizer=None, vector_config={'default': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.NONE: 'none'>, model={}, source_properties=None), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}),\n",
       " 'DocumentosPDFOllama': _CollectionConfigSimple(name='DocumentosPDFOllama', description=None, generative_config=None, properties=[_Property(name='text', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='chunk_id', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='source', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer=None, vectorizer_configs={}), _Property(name='length', description=\"This property was generated by Weaviate's auto-schema feature on Wed Aug 20 15:40:15 2025\", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer=None, vectorizer_configs={})], references=[], reranker_config=None, vectorizer_config=None, vectorizer=None, vector_config={'default': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.NONE: 'none'>, model={}, source_properties=None), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))})}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.collections.list_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30deb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = get_ollama_embedding(\"What is a RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d200863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:361: ResourceWarning: unclosed <socket.socket fd=580, family=23, type=1, proto=0, laddr=('::1', 51546, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  obj, end = self.scan_once(s, idx)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a Weaviate local\n",
      "üì¶ Conectado a la colecci√≥n 'DocumentosPDFOllama'\n",
      "üìÑ Cargando PDF: ./data/diego_velazquez.pdf\n",
      "üìù Documento dividido en 20 chunks\n",
      "üî¢ Comenzando desde chunk_id: 20\n",
      "üíæ Insertando documentos...\n",
      "‚úÖ 20 documentos insertados exitosamente\n",
      "üìä Total de documentos en la colecci√≥n: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jose\\Documents\\Programacion\\Chatbot_Langchain\\.venv\\Lib\\site-packages\\weaviate\\warnings.py:302: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/diego_velazquez.pdf\"\n",
    "load_new_file(file_path=file_path, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ba309",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(collection_name)\n",
    "if query_embedding:\n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=query_embedding,\n",
    "        limit=4,\n",
    "        return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìã Resultados con embeddings de Ollama:\")\n",
    "    print(f\"Objetos: {len(response.objects)}\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, obj in enumerate(response.objects, 1):\n",
    "        print(f\"\\nüî∏ Resultado {i}:\")\n",
    "        print(f\"   Distancia: {obj.metadata.distance:.4f}\")\n",
    "        print(f\"   Texto: {obj.properties['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3ddcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.data.sync._DataCollection at 0x1a966e47390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Carga las variables de entorno para las claves de API\n",
    "load_dotenv()\n",
    "\n",
    "# Configura tus claves de API\n",
    "# Aseg√∫rate de que OPENAI_API_KEY, WEAVIATE_URL y WEAVIATE_API_KEY est√©n en tu archivo .env\n",
    "\n",
    "# --- 1. Cargar documentos ---\n",
    "# Usaremos un archivo de texto simple como ejemplo\n",
    "# Crea un archivo llamado 'ejemplo.txt' con el texto que quieras\n",
    "# Por ejemplo: \"LangChain es un framework para desarrollar aplicaciones con LLMs. Weaviate es un motor de b√∫squeda de vectores...\"\n",
    "loader = TextLoader(\"ejemplo.txt\")\n",
    "documents = loader.load()\n",
    "print(f\"Documentos cargados: {len(documents)}\")\n",
    "\n",
    "# --- 2. Dividir los documentos ---\n",
    "# Esto es crucial para manejar textos largos.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Fragmentos creados: {len(docs)}\")\n",
    "\n",
    "# --- 3. Generar embeddings ---\n",
    "# Inicializa el modelo de embeddings de OpenAI\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# --- 4. Almacenar en Weaviate ---\n",
    "# Aseg√∫rate de que tu instancia de Weaviate est√© funcionando.\n",
    "# El nombre de la clase es el \"esquema\" para tus datos en Weaviate.\n",
    "index_name = \"LangChain_RAG_Demo\"\n",
    "\n",
    "# Crea la conexi√≥n con Weaviate y sube los documentos\n",
    "# Esto crear√° una nueva colecci√≥n (clase) en tu instancia de Weaviate\n",
    "weaviate_client = Weaviate.from_documents(\n",
    "    docs,\n",
    "    embeddings_model,\n",
    "    client=None, # Puedes pasar un cliente Weaviate ya inicializado aqu√≠\n",
    "    by_text=False, # Si los documentos ya tienen embeddings, usa True\n",
    "    weaviate_url=weaviate_url,\n",
    "    api_key=weaviate_api_key,\n",
    "    index_name=index_name\n",
    ")\n",
    "weaviate_client.\n",
    "\n",
    "print(f\"Documentos almacenados en Weaviate bajo la clase '{index_name}'.\")\n",
    "\n",
    "# --- 5. Crear el sistema RAG ---\n",
    "# Inicializa el modelo de lenguaje (LLM) que usar√° LangChain\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Crea el \"retriever\" (recuperador). LangChain usar√° este objeto\n",
    "# para buscar los documentos relevantes en Weaviate.\n",
    "retriever = weaviate_client.as_retriever()\n",
    "\n",
    "# Crea la cadena RAG. Le decimos que use el LLM para generar la respuesta\n",
    "# y el retriever para encontrar la informaci√≥n relevante.\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# --- Preguntar ---\n",
    "query = \"Expl√≠came qu√© es LangChain.\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "print(\"\\n--- Respuesta del sistema RAG ---\")\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
