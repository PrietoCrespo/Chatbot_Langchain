import requests
import json
import gradio as gr
from functions import count_tokens_simple, initialize_weaviate_and_get_retriever_with_images, summarize_conversation, tokenize_with_ollama
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# Configuraci칩n de memoria
MAX_TOKENS = 300  # L칤mite antes de hacer resumen
SUMMARY_SIZE = 500  # Tama침o aproximado del resumen

# Schema de funciones disponibles
FUNCTION_SCHEMA = {
    "suma": {
        "name": "suma",
        "description": "Suma dos n칰meros y devuelve el resultado",
        "parameters": {
            "type": "object",
            "properties": {
                "a": {
                    "type": "number",
                    "description": "Primer n칰mero a sumar"
                },
                "b": {
                    "type": "number", 
                    "description": "Segundo n칰mero a sumar"
                }
            },
            "required": ["a", "b"]
        }
    }
}

def execute_function(function_name, parameters):
    """Ejecuta una funci칩n basada en el nombre y par치metros proporcionados"""
    if function_name == "suma":
        try:
            a = parameters.get("a", 0)
            b = parameters.get("b", 0)
            result = a + b
            return {
                "success": True,
                "result": result,
                "message": f"La suma de {a} + {b} = {result}"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Error al ejecutar la funci칩n suma"
            }
    else:
        return {
            "success": False,
            "error": f"Funci칩n '{function_name}' no encontrada",
            "message": "La funci칩n solicitada no est치 disponible"
        }

def parse_function_call(response_text):
    """
    Intenta parsear una respuesta del modelo para detectar llamadas a funciones
    Busca patrones JSON que contengan function_call
    """
    try:
        # Buscar patrones JSON en la respuesta
        import re
        json_pattern = r'\{[^{}]*"function_call"[^{}]*\}'
        matches = re.findall(json_pattern, response_text)
        
        if matches:
            # Intentar parsear el primer match como JSON
            function_call_data = json.loads(matches[0])
            return function_call_data
        
        # Tambi칠n intentar parsear toda la respuesta como JSON
        try:
            full_json = json.loads(response_text.strip())
            if "function_call" in full_json:
                return full_json
        except:
            pass
            
        return None
    except Exception as e:
        print(f"Error parseando function call: {e}")
        return None

def manage_memory(llm, chat_history):
    """Gestiona la memoria din치mica con resumen autom치tico"""
    # Contar tokens actuales
    current_tokens = count_tokens_simple(chat_history)
    print(f"Tokens actuales: {current_tokens}")  # Para debug
    
    # Si excede el l칤mite, hacer resumen
    if current_tokens > MAX_TOKENS:
        print("L칤mite de tokens excedido. Resumiendo conversaci칩n...")
        summarized_history = summarize_conversation(llm, chat_history)
        return summarized_history
    
    return chat_history

def initialize_rag():
    retriever = initialize_weaviate_and_get_retriever_with_images(data_folder='./data',collection_name='DocumentosPDFOllama')
    llm = OllamaLLM(model="qwen3:4b")

    contextualize_q_system_prompt = """Reformula la pregunta para que se entienda sin necesidad del historial previo. No respondas, solo reformula si es necesario."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # Crear el prompt del sistema con llaves escapadas
    system_prompt = """Eres un asistente que responde preguntas bas치ndose en documentos PDF que pueden incluir texto e im치genes. 

    FUNCIONES DISPONIBLES:
    - suma(a, b): Suma dos n칰meros y devuelve el resultado
    - Par치metros: a (n칰mero), b (n칰mero)
    - Ejemplo de uso: para sumar n칰meros

    INSTRUCCIONES:
    1. Responde 칰nicamente usando la informaci칩n del contexto proporcionado para preguntas sobre documentos.
    2. Si la pregunta no puede responderse con la informaci칩n disponible, di que no tienes esa informaci칩n en los documentos.
    3. Si necesitas realizar c치lculos matem치ticos (sumas), puedes usar las funciones disponibles.
    4. Para usar una funci칩n, responde 칔NICAMENTE con un JSON en este formato:
    {{
        "function_call": {{
        "name": "nombre_funcion",
        "parameters": {{
            "parametro1": valor1,
            "parametro2": valor2
        }}
        }}
    }}
    5. Si no necesitas usar ninguna funci칩n, responde normalmente con texto.

    Contexto de documentos: {context}"""
    
    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    stuff_documents_chain = create_stuff_documents_chain(llm, answer_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, stuff_documents_chain)

    return rag_chain, llm

def process_question(message, history):
    """
    Procesa una pregunta usando el sistema RAG y mantiene el historial con memoria din치mica
    Incluye soporte para function calling
    """
    global chat_history, llm
    
    try:
        # Gestionar memoria antes de procesar la pregunta
        print(f"Gestionando memoria...")
        chat_history = manage_memory(llm, chat_history)
        
        # Invocar el sistema RAG
        response = rag_chain.invoke({
            "input": message,
            "chat_history": chat_history
        })
        
        # Procesar la respuesta (eliminar etiquetas de pensamiento si existen)
        respuesta = response['answer']
        if '</think>' in respuesta:
            respuesta = respuesta.split('</think>')[-1].strip()
        
        print(f"Respuesta del modelo: {respuesta}")
        
        # Verificar si es una llamada a funci칩n
        function_call_data = parse_function_call(respuesta)
        
        if function_call_data and "function_call" in function_call_data:
            # Ejecutar la funci칩n
            func_call = function_call_data["function_call"]
            function_name = func_call.get("name")
            parameters = func_call.get("parameters", {})
            
            print(f"Ejecutando funci칩n: {function_name} con par치metros: {parameters}")
            
            # Ejecutar la funci칩n
            function_result = execute_function(function_name, parameters)
            
            if function_result["success"]:
                respuesta_final = function_result["message"]
                # Agregar informaci칩n adicional si es 칰til
                respuesta_final += f"\n\n(Resultado calculado: {function_result['result']})"
            else:
                respuesta_final = f"Error en la ejecuci칩n: {function_result['message']}"
            
            # Actualizar el historial con la pregunta, la funci칩n ejecutada y el resultado
            chat_history.extend([
                HumanMessage(content=message),
                AIMessage(content=f"Funci칩n ejecutada: {function_name}({parameters}) = {function_result.get('result', 'error')}")
            ])
            
            return respuesta_final
        else:
            # Respuesta normal sin function calling
            chat_history.extend([
                HumanMessage(content=message), 
                AIMessage(content=respuesta)
            ])
            
            return respuesta
        
    except Exception as e:
        error_msg = f"Error al procesar la pregunta: {str(e)}"
        print(error_msg)
        return error_msg
    
def clear_chat():
    """Limpia el historial de chat"""
    global chat_history
    chat_history = []
    return None

def get_memory_stats():
    """Obtiene estad칤sticas de la memoria actual"""
    global chat_history
    tokens = count_tokens_simple(chat_history)
    messages = len(chat_history)
    return f"Tokens: {tokens}/{MAX_TOKENS} | Mensajes: {messages}"

# Crear la interfaz de Gradio
with gr.Blocks(title="Sistema RAG con Function Calling", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# 游뱄 Sistema RAG con Function Calling")
    gr.Markdown("Haz preguntas sobre tus documentos y realiza c치lculos matem치ticos. El modelo puede usar funciones autom치ticamente.")
    
    # Mostrar funciones disponibles
    gr.Markdown("### Funciones disponibles:")
    gr.Markdown("- **suma(a, b)**: Suma dos n칰meros")
    gr.Markdown("*Ejemplos: '쮺u치nto es 15 + 27?', 'Suma 100 y 250', 'Calcula 45 + 33'*")
    
    # Indicador de memoria
    memory_info = gr.Textbox(
        label="Estado de la Memoria",
        value="Tokens: 0/300 | Mensajes: 0",
        interactive=False,
        max_lines=1
    )
    
    chatbot = gr.Chatbot(
        height=500,
        placeholder="Las respuestas aparecer치n aqu칤...",
        label="Conversaci칩n"
    )
    
    with gr.Row():
        msg = gr.Textbox(
            placeholder="Escribe tu pregunta aqu칤... (documentos o c치lculos)",
            label="Tu pregunta",
            scale=4
        )
        submit_btn = gr.Button("Enviar", variant="primary", scale=1)
    
    with gr.Row():
        clear_btn = gr.Button("Limpiar conversaci칩n", variant="secondary")
        refresh_memory_btn = gr.Button("Actualizar memoria", variant="secondary")
    
    # Configurar los eventos
    def respond(message, chat_history_gradio):
        if not message.strip():
            return chat_history_gradio, "", get_memory_stats()
        
        # Procesar la pregunta
        bot_response = process_question(message, chat_history_gradio)
        
        # Actualizar el historial de Gradio
        chat_history_gradio.append((message, bot_response))
        
        return chat_history_gradio, "", get_memory_stats()
    
    def clear_conversation():
        clear_chat()
        return [], "", "Tokens: 0/300 | Mensajes: 0"
    
    def refresh_memory_stats():
        return get_memory_stats()
    
    # Eventos de los botones
    submit_btn.click(
        respond, 
        inputs=[msg, chatbot], 
        outputs=[chatbot, msg, memory_info]
    )
    
    msg.submit(
        respond, 
        inputs=[msg, chatbot], 
        outputs=[chatbot, msg, memory_info]
    )
    
    clear_btn.click(
        clear_conversation,
        outputs=[chatbot, msg, memory_info]
    )
    
    refresh_memory_btn.click(
        refresh_memory_stats,
        outputs=[memory_info]
    )

if __name__ == "__main__":
    
    rag_chain, llm = initialize_rag()  # Ahora retorna tambi칠n el LLM

    chat_history = []
    demo.launch(
        server_name="0.0.0.0",  # Permite acceso desde otras IPs
        server_port=7860,       # Puerto por defecto de Gradio
        share=False,            # Cambiar a True si quieres un enlace p칰blico
        debug=True              # Habilita el modo debug
    )